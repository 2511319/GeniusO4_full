# backend/routers/analysis.py
"""
–†–æ—É—Ç–µ—Ä –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç
–û—Å–Ω–æ–≤–Ω–æ–π endpoint –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è 24 –æ–±—ä–µ–∫—Ç–æ–≤ AI –∞–Ω–∞–ª–∏–∑–∞
"""

import time
import json
from typing import Dict, Any, Optional
from datetime import datetime

from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks
from pydantic import BaseModel, validator

from auth.dependencies import get_current_user, check_analysis_limit
from services.crypto_compare_provider import CryptoCompareProvider
from services.chatgpt_analyzer import ChatGPTAnalyzer
from services.cache_service import CacheService
from config.config import get_settings, logger, Constants
from config.database import execute_query, execute_one

router = APIRouter()
settings = get_settings()


class AnalysisRequest(BaseModel):
    """–ú–æ–¥–µ–ª—å –∑–∞–ø—Ä–æ—Å–∞ –∞–Ω–∞–ª–∏–∑–∞"""
    symbol: str = "BTCUSDT"
    interval: str = "4h"
    days: int = 15
    
    @validator('symbol')
    def validate_symbol(cls, v):
        if v not in Constants.SUPPORTED_SYMBOLS:
            raise ValueError(f'–°–∏–º–≤–æ–ª –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ–¥–Ω–∏–º –∏–∑: {Constants.SUPPORTED_SYMBOLS}')
        return v.upper()
    
    @validator('interval')
    def validate_interval(cls, v):
        if v not in Constants.SUPPORTED_INTERVALS:
            raise ValueError(f'–ò–Ω—Ç–µ—Ä–≤–∞–ª –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ–¥–Ω–∏–º –∏–∑: {Constants.SUPPORTED_INTERVALS}')
        return v
    
    @validator('days')
    def validate_days(cls, v):
        if not 7 <= v <= 30:
            raise ValueError('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ—Ç 7 –¥–æ 30')
        return v


class AnalysisResponse(BaseModel):
    """–ú–æ–¥–µ–ª—å –æ—Ç–≤–µ—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞"""
    success: bool
    analysis_id: Optional[int] = None
    symbol: str
    interval: str
    days: int
    processing_time_ms: int
    ai_model: str
    timestamp: str
    data: Dict[str, Any]  # 24 –æ–±—ä–µ–∫—Ç–∞ AI –∞–Ω–∞–ª–∏–∑–∞


@router.post("/analyze", response_model=AnalysisResponse)
async def analyze_symbol(
    request: AnalysisRequest,
    background_tasks: BackgroundTasks,
    current_user: Dict[str, Any] = Depends(get_current_user),
    _: None = Depends(check_analysis_limit)
):
    """
    –û—Å–Ω–æ–≤–Ω–æ–π endpoint –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç 24 –æ–±—ä–µ–∫—Ç–∞ AI –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–≥–ª–∞—Å–Ω–æ chatgpt_response_1749154674.json
    """
    start_time = time.time()
    
    try:
        logger.info(f"üîç –ù–∞—á–∞—Ç –∞–Ω–∞–ª–∏–∑ {request.symbol} {request.interval} –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {current_user['telegram_id']}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_service = CacheService()
        cache_key = f"analysis:{request.symbol}:{request.interval}:{request.days}"
        
        if settings.enable_cache:
            cached_result = await cache_service.get(cache_key)
            if cached_result:
                logger.info(f"üì¶ –í–æ–∑–≤—Ä–∞—â–µ–Ω –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è {request.symbol}")
                return AnalysisResponse(
                    success=True,
                    symbol=request.symbol,
                    interval=request.interval,
                    days=request.days,
                    processing_time_ms=int((time.time() - start_time) * 1000),
                    ai_model=settings.openai_model,
                    timestamp=datetime.now().isoformat(),
                    data=cached_result
                )
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ OHLCV –¥–∞–Ω–Ω—ã—Ö
        crypto_provider = CryptoCompareProvider()
        ohlcv_data = await crypto_provider.get_ohlcv_data(
            symbol=request.symbol,
            interval=request.interval,
            limit=_calculate_limit(request.interval, request.days)
        )
        
        if not ohlcv_data or len(ohlcv_data) < 50:
            raise HTTPException(
                status_code=400,
                detail=f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ {request.symbol}"
            )
        
        logger.info(f"üìä –ü–æ–ª—É—á–µ–Ω–æ {len(ohlcv_data)} —Å–≤–µ—á–µ–π –¥–ª—è {request.symbol}")
        
        # AI –∞–Ω–∞–ª–∏–∑
        analyzer = ChatGPTAnalyzer()
        analysis_result = await analyzer.analyze_ohlcv_data(
            ohlcv_data=ohlcv_data,
            symbol=request.symbol,
            interval=request.interval
        )
        
        if not analysis_result:
            raise HTTPException(
                status_code=500,
                detail="–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ AI –∞–Ω–∞–ª–∏–∑–∞"
            )
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –æ—Ç–≤–µ—Ç–∞ (24 –æ–±—ä–µ–∫—Ç–∞)
        if not _validate_analysis_structure(analysis_result):
            logger.error("‚ùå AI –∞–Ω–∞–ª–∏–∑ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ 24 –æ–±—ä–µ–∫—Ç–∞")
            raise HTTPException(
                status_code=500,
                detail="–ù–µ–ø–æ–ª–Ω—ã–π AI –∞–Ω–∞–ª–∏–∑"
            )
        
        processing_time_ms = int((time.time() - start_time) * 1000)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö (–≤ —Ñ–æ–Ω–µ)
        background_tasks.add_task(
            _save_analysis_to_db,
            user_id=current_user['id'],
            symbol=request.symbol,
            interval=request.interval,
            days=request.days,
            analysis_data=analysis_result,
            ohlcv_data=ohlcv_data,
            processing_time_ms=processing_time_ms
        )
        
        # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        if settings.enable_cache:
            background_tasks.add_task(
                cache_service.set,
                cache_key,
                analysis_result,
                settings.cache_ttl_seconds
            )
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—á–µ—Ç—á–∏–∫–∞ –∞–Ω–∞–ª–∏–∑–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        background_tasks.add_task(
            _update_user_analysis_count,
            current_user['id']
        )
        
        logger.info(f"‚úÖ –ê–Ω–∞–ª–∏–∑ {request.symbol} –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {processing_time_ms}ms")
        
        return AnalysisResponse(
            success=True,
            symbol=request.symbol,
            interval=request.interval,
            days=request.days,
            processing_time_ms=processing_time_ms,
            ai_model=settings.openai_model,
            timestamp=datetime.now().isoformat(),
            data=analysis_result
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {request.symbol}: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {str(e)}"
        )


@router.get("/history")
async def get_analysis_history(
    limit: int = 10,
    offset: int = 0,
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –∞–Ω–∞–ª–∏–∑–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    try:
        query = """
            SELECT id, symbol, interval_type, days_count, 
                   created_at, processing_time_ms
            FROM analyses 
            WHERE user_id = :user_id 
            ORDER BY created_at DESC 
            OFFSET :offset ROWS FETCH NEXT :limit ROWS ONLY
        """
        
        results = await execute_query(query, {
            "user_id": current_user['id'],
            "offset": offset,
            "limit": min(limit, 50)  # –ú–∞–∫—Å–∏–º—É–º 50 –∑–∞–ø–∏—Å–µ–π
        })
        
        history = []
        for row in results:
            history.append({
                "id": row[0],
                "symbol": row[1],
                "interval": row[2],
                "days": row[3],
                "created_at": row[4].isoformat() if row[4] else None,
                "processing_time_ms": row[5]
            })
        
        return {
            "success": True,
            "history": history,
            "total": len(history)
        }
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏")


@router.get("/history/{analysis_id}")
async def get_analysis_by_id(
    analysis_id: int,
    current_user: Dict[str, Any] = Depends(get_current_user)
):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ ID"""
    try:
        query = """
            SELECT analysis_data, symbol, interval_type, days_count, 
                   created_at, processing_time_ms
            FROM analyses 
            WHERE id = :analysis_id AND user_id = :user_id
        """
        
        result = await execute_one(query, {
            "analysis_id": analysis_id,
            "user_id": current_user['id']
        })
        
        if not result:
            raise HTTPException(status_code=404, detail="–ê–Ω–∞–ª–∏–∑ –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        analysis_data = json.loads(result[0]) if result[0] else {}
        
        return {
            "success": True,
            "analysis": {
                "id": analysis_id,
                "data": analysis_data,
                "symbol": result[1],
                "interval": result[2],
                "days": result[3],
                "created_at": result[4].isoformat() if result[4] else None,
                "processing_time_ms": result[5]
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞ {analysis_id}: {e}")
        raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞")


def _calculate_limit(interval: str, days: int) -> int:
    """–†–∞—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–≤–µ—á–µ–π –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""
    intervals_per_day = {
        "1m": 1440, "5m": 288, "15m": 96, "30m": 48,
        "1h": 24, "2h": 12, "4h": 6, "6h": 4, "8h": 3, "12h": 2, "1d": 1
    }
    
    per_day = intervals_per_day.get(interval, 6)  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 4h
    limit = per_day * days
    
    return min(max(limit, Constants.MIN_LIMIT), Constants.MAX_LIMIT)


def _validate_analysis_structure(analysis: Dict[str, Any]) -> bool:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã AI –∞–Ω–∞–ª–∏–∑–∞ (24 –æ–±—ä–µ–∫—Ç–∞)"""
    required_keys = [
        "primary_analysis", "confidence_in_trading_decisions", "support_resistance_levels",
        "trend_lines", "pivot_points", "fibonacci_levels", "volume_analysis",
        "momentum_indicators", "oscillators", "moving_averages", "bollinger_bands",
        "ichimoku_cloud", "candlestick_patterns", "chart_patterns", "elliott_wave",
        "market_structure", "risk_management", "entry_exit_points", "price_targets",
        "stop_loss_levels", "market_sentiment", "correlation_analysis", 
        "volatility_analysis", "time_frame_analysis"
    ]
    
    missing_keys = [key for key in required_keys if key not in analysis]
    
    if missing_keys:
        logger.warning(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–ª—é—á–∏ –≤ AI –∞–Ω–∞–ª–∏–∑–µ: {missing_keys}")
        return False
    
    return True


async def _save_analysis_to_db(
    user_id: int,
    symbol: str,
    interval: str,
    days: int,
    analysis_data: Dict[str, Any],
    ohlcv_data: list,
    processing_time_ms: int
):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
    try:
        query = """
            INSERT INTO analyses (
                user_id, symbol, interval_type, days_count, 
                analysis_data, ohlcv_data, ai_model, processing_time_ms
            ) VALUES (
                :user_id, :symbol, :interval_type, :days_count,
                :analysis_data, :ohlcv_data, :ai_model, :processing_time_ms
            )
        """
        
        await execute_query(query, {
            "user_id": user_id,
            "symbol": symbol,
            "interval_type": interval,
            "days_count": days,
            "analysis_data": json.dumps(analysis_data, ensure_ascii=False),
            "ohlcv_data": json.dumps(ohlcv_data, ensure_ascii=False),
            "ai_model": settings.openai_model,
            "processing_time_ms": processing_time_ms
        })
        
        logger.info(f"üíæ –ê–Ω–∞–ª–∏–∑ {symbol} —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –ë–î –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞ –≤ –ë–î: {e}")


async def _update_user_analysis_count(user_id: int):
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—á–µ—Ç—á–∏–∫–∞ –∞–Ω–∞–ª–∏–∑–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    try:
        query = """
            UPDATE users 
            SET analyses_today = analyses_today + 1,
                last_analysis_date = CURRENT_DATE
            WHERE id = :user_id
        """
        
        await execute_query(query, {"user_id": user_id})
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—á–µ—Ç—á–∏–∫–∞ –∞–Ω–∞–ª–∏–∑–æ–≤: {e}")
