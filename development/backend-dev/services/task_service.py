# ‚ö° Task Service for ChartGenius
# –í–µ—Ä—Å–∏—è: 1.1.0-dev
# Background tasks —Å Celery –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏

import os
import asyncio
import json
from typing import Dict, Any, List, Optional
from celery import Celery
from celery.result import AsyncResult
import redis.asyncio as redis
from datetime import datetime, timedelta
import logging
from backend.services.llm_service import LLMService
from backend.services.metrics_service import metrics

logger = logging.getLogger(__name__)

# === CELERY CONFIGURATION ===
celery_app = Celery(
    'chartgenius',
    broker='redis://localhost:6379/0',
    backend='redis://localhost:6379/0',
    include=['backend.services.task_service']
)

celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
    task_track_started=True,
    task_time_limit=300,  # 5 –º–∏–Ω—É—Ç –º–∞–∫—Å–∏–º—É–º
    task_soft_time_limit=240,  # 4 –º–∏–Ω—É—Ç—ã soft limit
    worker_prefetch_multiplier=1,
    worker_max_tasks_per_child=1000,
)

# === TASK STATUS ENUM ===
class TaskStatus:
    PENDING = 'PENDING'
    STARTED = 'STARTED'
    PROCESSING = 'PROCESSING'
    SUCCESS = 'SUCCESS'
    FAILURE = 'FAILURE'
    RETRY = 'RETRY'

# === BACKGROUND TASKS ===
@celery_app.task(bind=True, name='process_analysis')
def process_analysis_task(self, symbol: str, interval: str, layers: List[str], 
                         user_id: str, task_metadata: Dict[str, Any] = None):
    """
    –§–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç
    
    Args:
        symbol: –°–∏–º–≤–æ–ª –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, BTCUSDT)
        interval: –í—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª (–Ω–∞–ø—Ä–∏–º–µ—Ä, 4h)
        layers: –°–ø–∏—Å–æ–∫ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        task_metadata: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    """
    task_id = self.request.id
    start_time = datetime.utcnow()
    
    try:
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏
        self.update_state(
            state=TaskStatus.STARTED,
            meta={
                'status': '–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞...',
                'progress': 0,
                'started_at': start_time.isoformat()
            }
        )
        
        logger.info(f"Starting analysis task {task_id} for {symbol} {interval}")
        
        # === STEP 1: –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ===
        self.update_state(
            state=TaskStatus.PROCESSING,
            meta={
                'status': '–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...',
                'progress': 20,
                'current_step': 'market_data'
            }
        )
        _notify_task_progress(task_id, 20, '–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...')

        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ —Ä—ã–Ω–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        try:
            from backend.services.market_data_service import market_data_service
            df = await market_data_service.get_ohlcv_data(symbol, interval, 500)

            if df is not None and not df.empty:
                market_data = {
                    'symbol': symbol,
                    'interval': interval,
                    'data': df.to_dict('records'),
                    'count': len(df),
                    'source': 'market_data_service'
                }
            else:
                # Fallback –Ω–∞ —Å–∏–º—É–ª—è—Ü–∏—é –µ—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö
                market_data = _simulate_market_data_fetch(symbol, interval)

        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–º—É–ª—è—Ü–∏—é")
            market_data = _simulate_market_data_fetch(symbol, interval)

        # === STEP 2: –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ ===
        self.update_state(
            state=TaskStatus.PROCESSING,
            meta={
                'status': '–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤...',
                'progress': 40,
                'current_step': 'indicators',
                'indicators': layers
            }
        )
        _notify_task_progress(task_id, 40, '–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤...')

        # indicators = await calculate_indicators(market_data, layers)
        indicators = _simulate_indicators_calculation(market_data, layers)

        # === STEP 3: LLM –∞–Ω–∞–ª–∏–∑ ===
        self.update_state(
            state=TaskStatus.PROCESSING,
            meta={
                'status': '–ì–µ–Ω–µ—Ä–∞—Ü–∏—è AI –∞–Ω–∞–ª–∏–∑–∞...',
                'progress': 60,
                'current_step': 'llm_analysis'
            }
        )
        _notify_task_progress(task_id, 60, '–ì–µ–Ω–µ—Ä–∞—Ü–∏—è AI –∞–Ω–∞–ª–∏–∑–∞...')
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é LLM –¥–ª—è Celery
        analysis_result = _generate_llm_analysis(symbol, interval, market_data, indicators)
        
        # === STEP 4: –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ ===
        self.update_state(
            state=TaskStatus.PROCESSING,
            meta={
                'status': '–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...',
                'progress': 80,
                'current_step': 'finalization'
            }
        )
        _notify_task_progress(task_id, 80, '–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...')
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        final_result = {
            'symbol': symbol,
            'interval': interval,
            'layers': layers,
            'market_data': market_data,
            'indicators': indicators,
            'analysis': analysis_result,
            'metadata': {
                'task_id': task_id,
                'user_id': user_id,
                'created_at': start_time.isoformat(),
                'completed_at': datetime.utcnow().isoformat(),
                'processing_time_seconds': (datetime.utcnow() - start_time).total_seconds()
            }
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ Redis —Å TTL
        _save_analysis_result(task_id, final_result)
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
        _notify_user_analysis_complete(user_id, task_id, symbol)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics.track_user_action('analysis_completed', 'user')
        
        logger.info(f"Analysis task {task_id} completed successfully")
        
        return {
            'status': 'completed',
            'result': final_result,
            'progress': 100
        }
        
    except Exception as e:
        logger.error(f"Analysis task {task_id} failed: {e}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –æ—à–∏–±–æ–∫
        metrics.track_error(type(e).__name__, 'analysis_task')
        
        self.update_state(
            state=TaskStatus.FAILURE,
            meta={
                'status': f'–û—à–∏–±–∫–∞: {str(e)}',
                'error': str(e),
                'failed_at': datetime.utcnow().isoformat()
            }
        )
        
        raise

@celery_app.task(bind=True, name='cleanup_old_tasks')
def cleanup_old_tasks(self):
    """–û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∑–∞–¥–∞—á –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    try:
        # –û—á–∏—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ç–∞—Ä—à–µ 24 —á–∞—Å–æ–≤
        cutoff_time = datetime.utcnow() - timedelta(hours=24)
        
        # –ó–¥–µ—Å—å –±—É–¥–µ—Ç –ª–æ–≥–∏–∫–∞ –æ—á–∏—Å—Ç–∫–∏ Redis
        logger.info("Cleanup task completed")
        
        return {'status': 'completed', 'cleaned_tasks': 0}
        
    except Exception as e:
        logger.error(f"Cleanup task failed: {e}")
        raise

# === HELPER FUNCTIONS ===
def _simulate_market_data_fetch(symbol: str, interval: str) -> Dict[str, Any]:
    """–°–∏–º—É–ª—è—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    import time
    time.sleep(2)  # –ò–º–∏—Ç–∞—Ü–∏—è API –∑–∞–ø—Ä–æ—Å–∞
    
    return {
        'symbol': symbol,
        'interval': interval,
        'data': [
            {'timestamp': '2025-06-25T10:00:00Z', 'open': 50000, 'high': 51000, 'low': 49500, 'close': 50500, 'volume': 1000},
            {'timestamp': '2025-06-25T14:00:00Z', 'open': 50500, 'high': 52000, 'low': 50000, 'close': 51500, 'volume': 1200},
        ],
        'count': 2
    }

def _simulate_indicators_calculation(market_data: Dict[str, Any], layers: List[str]) -> Dict[str, Any]:
    """–°–∏–º—É–ª—è—Ü–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
    import time
    time.sleep(1)  # –ò–º–∏—Ç–∞—Ü–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
    
    indicators = {}
    for layer in layers:
        if layer == 'RSI':
            indicators['RSI'] = [45.2, 52.8]
        elif layer == 'MACD':
            indicators['MACD'] = [0.12, 0.18]
        elif layer == 'MA_20':
            indicators['MA_20'] = [50250, 51000]
    
    return indicators

def _generate_llm_analysis(symbol: str, interval: str, market_data: Dict[str, Any], 
                          indicators: Dict[str, Any]) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è LLM –∞–Ω–∞–ª–∏–∑–∞ (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è Celery)"""
    import time
    time.sleep(5)  # –ò–º–∏—Ç–∞—Ü–∏—è LLM –∑–∞–ø—Ä–æ—Å–∞
    
    # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç –≤—ã–∑–æ–≤ LLM
    analysis = f"""
    –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ {symbol} –Ω–∞ –∏–Ω—Ç–µ—Ä–≤–∞–ª–µ {interval}:
    
    üìä –¢–µ–∫—É—â–∞—è —Å–∏—Ç—É–∞—Ü–∏—è:
    - –¶–µ–Ω–∞ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –±–æ–∫–æ–≤–æ–º —Ç—Ä–µ–Ω–¥–µ
    - RSI –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    - –û–±—ä–µ–º—ã —Ç–æ—Ä–≥–æ–≤ —É–º–µ—Ä–µ–Ω–Ω—ã–µ
    
    üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
    - –û–∂–∏–¥–∞–Ω–∏–µ –ø—Ä–æ–±–æ—è –∫–ª—é—á–µ–≤—ã—Ö —É—Ä–æ–≤–Ω–µ–π
    - –†–∏—Å–∫-–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω
    
    ‚ö†Ô∏è –†–∏—Å–∫–∏: –°—Ä–µ–¥–Ω–∏–µ
    """
    
    return analysis.strip()

def _save_analysis_result(task_id: str, result: Dict[str, Any]):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤ Redis"""
    try:
        import redis
        r = redis.Redis(host='localhost', port=6379, decode_responses=True)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å TTL 24 —á–∞—Å–∞
        r.setex(f"analysis_result:{task_id}", 86400, json.dumps(result))
        
        logger.info(f"Analysis result saved for task {task_id}")
        
    except Exception as e:
        logger.error(f"Failed to save analysis result: {e}")

def _notify_user_analysis_complete(user_id: str, task_id: str, symbol: str):
    """–£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –∞–Ω–∞–ª–∏–∑–∞"""
    try:
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Redis pub/sub –¥–ª—è WebSocket
        import redis
        r = redis.Redis(host='localhost', port=6379, decode_responses=True)

        notification_data = {
            'type': 'personal',
            'user_id': user_id,
            'message': {
                'type': 'analysis_completed',
                'task_id': task_id,
                'symbol': symbol,
                'message': f'–ê–Ω–∞–ª–∏–∑ {symbol} –∑–∞–≤–µ—Ä—à–µ–Ω',
                'timestamp': datetime.utcnow().isoformat(),
                'action': {
                    'type': 'view_result',
                    'url': f'/analysis/result/{task_id}'
                }
            }
        }

        r.publish('chartgenius:notifications', json.dumps(notification_data))
        logger.info(f"WebSocket notification sent to user {user_id} for task {task_id}")

    except Exception as e:
        logger.error(f"Failed to notify user: {e}")

def _notify_task_progress(task_id: str, progress: int, status: str):
    """–£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ –∑–∞–¥–∞—á–∏"""
    try:
        import redis
        r = redis.Redis(host='localhost', port=6379, decode_responses=True)

        notification_data = {
            'type': 'task_update',
            'task_id': task_id,
            'data': {
                'progress': progress,
                'status': status,
                'timestamp': datetime.utcnow().isoformat()
            }
        }

        r.publish('chartgenius:notifications', json.dumps(notification_data))
        logger.debug(f"Task progress notification sent for {task_id}: {progress}%")

    except Exception as e:
        logger.error(f"Failed to notify task progress: {e}")

# === TASK MANAGEMENT CLASS ===
class TaskManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ñ–æ–Ω–æ–≤—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏"""
    
    def __init__(self):
        self.redis_client = None
    
    async def get_redis(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ Redis –∫–ª–∏–µ–Ω—Ç–∞"""
        if not self.redis_client:
            self.redis_client = redis.Redis(
                host='localhost', 
                port=6379, 
                decode_responses=True
            )
        return self.redis_client
    
    async def start_analysis_task(self, symbol: str, interval: str, layers: List[str], 
                                 user_id: str) -> str:
        """–ó–∞–ø—É—Å–∫ –∑–∞–¥–∞—á–∏ –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            task = process_analysis_task.delay(symbol, interval, layers, user_id)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–¥–∞—á–µ
            task_info = {
                'task_id': task.id,
                'symbol': symbol,
                'interval': interval,
                'layers': layers,
                'user_id': user_id,
                'status': TaskStatus.PENDING,
                'created_at': datetime.utcnow().isoformat()
            }
            
            redis_client = await self.get_redis()
            await redis_client.setex(
                f"task_info:{task.id}", 
                86400,  # 24 —á–∞—Å–∞ TTL
                json.dumps(task_info)
            )
            
            logger.info(f"Analysis task started: {task.id}")
            return task.id
            
        except Exception as e:
            logger.error(f"Failed to start analysis task: {e}")
            raise
    
    async def get_task_status(self, task_id: str) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á–∏"""
        try:
            result = AsyncResult(task_id, app=celery_app)
            
            task_info = {
                'task_id': task_id,
                'status': result.status,
                'result': result.result if result.ready() else None,
                'info': result.info if hasattr(result, 'info') else None
            }
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ Redis
            redis_client = await self.get_redis()
            stored_info = await redis_client.get(f"task_info:{task_id}")
            if stored_info:
                stored_data = json.loads(stored_info)
                task_info.update(stored_data)
            
            return task_info
            
        except Exception as e:
            logger.error(f"Failed to get task status: {e}")
            return {
                'task_id': task_id,
                'status': 'ERROR',
                'error': str(e)
            }
    
    async def cancel_task(self, task_id: str) -> bool:
        """–û—Ç–º–µ–Ω–∞ –∑–∞–¥–∞—á–∏"""
        try:
            celery_app.control.revoke(task_id, terminate=True)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –≤ Redis
            redis_client = await self.get_redis()
            task_info = await redis_client.get(f"task_info:{task_id}")
            if task_info:
                data = json.loads(task_info)
                data['status'] = 'CANCELLED'
                data['cancelled_at'] = datetime.utcnow().isoformat()
                await redis_client.setex(f"task_info:{task_id}", 86400, json.dumps(data))
            
            logger.info(f"Task cancelled: {task_id}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to cancel task: {e}")
            return False

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –∑–∞–¥–∞—á
task_manager = TaskManager()
